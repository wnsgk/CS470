{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CS 470 Assignment 1**\n",
        "### Implementing a Multi-layer Perceptron (MLP)\n",
        "\n",
        "#### In this exercise, you will develop a neural network with fully-connected layers to perform image classification, and test it out on the [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset. "
      ],
      "metadata": {
        "id": "UEXqCDmc4f6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data\n"
      ],
      "metadata": {
        "id": "c9fNcMT74mZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xzvf cifar-10-python.tar.gz\n",
        "!rm cifar-10-python.tar.gz \n",
        "!pip install scipy"
      ],
      "metadata": {
        "id": "gs-k8OOJEsED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the data"
      ],
      "metadata": {
        "id": "rK9eUV4V-XaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib.pyplot import imread\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = pickle.load(f, encoding='latin1')\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)    \n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte"
      ],
      "metadata": {
        "id": "_aaj2Q_iE356"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the MLP classifier.   \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = '/content/cifar-10-batches-py'\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "        \n",
        "    # Subsample the data\n",
        "    mask = range(num_training, num_training + num_validation)\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = range(num_training)\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = range(num_test)\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis=0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "\n",
        "    # Reshape the data\n",
        "    X_train = X_train.reshape(num_training, -1)\n",
        "    X_val = X_val.reshape(num_validation, -1)\n",
        "    X_test = X_test.reshape(num_test, -1)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "metadata": {
        "id": "JLHp9_eOFS8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####*In* this work, fully-connected (linear) layers are used, i.e. the layers can be defined as $f(x) = Wx + b$ where $W$ is the weight matrix of the layer, $x$ is its input, and $b$ is the bias. The ReLU function is essentially defined as $f(x) = \\max(0, x)$.\n"
      ],
      "metadata": {
        "id": "NcgzT85a4zMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1./(1.+np.exp(-x))\n",
        "\n",
        "class MLP(object):\n",
        "  \"\"\"\n",
        "  A multi-layer fully-connected neural network has an input dimension of\n",
        "  d, a hidden layer dimension of h, and performs classification over c classes.\n",
        "  You must train the network with a softmax loss function and L1 regularization on the\n",
        "  weight matrices. The network uses a LeakyReLU nonlinearity after the first fully\n",
        "  -connected layer.\n",
        "\n",
        "  The network has the following architecture:\n",
        "\n",
        "  Input - Linear layer - LeakyReLU - Linear layer - Softmax\n",
        "\n",
        "  The outputs of the network are the labels for each class.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, activation, std=1e-4):\n",
        "    \"\"\"\n",
        "    An initialization function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size: integer\n",
        "        the dimension d of the input data.                          \n",
        "    hidden_size: integer\n",
        "        the number of neurons h in the hidden layer.               \n",
        "    output_size: integer\n",
        "        the number of classes c.                                   \n",
        "    activation: string\n",
        "        activation method name\n",
        "    std: float\n",
        "        standard deviation\n",
        "    \"\"\"\n",
        "    # w1: weights for the first linear layer                                    \n",
        "    # b1: biases for the first linear layer                                     \n",
        "    # w2: weights for the second linear layer                                   \n",
        "    # b2: biases for the second linear layer                                    \n",
        "\n",
        "    self.params = {}\n",
        "    self.params['w1'] = std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['w2'] = std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    self.leaky_relu_c = 0.01\n",
        "    self.activation_method = ['ReLU','LeakyReLU','SWISH'].index(activation)\n",
        "    print(\"Selected using \"+['ReLU','LeakyReLU','SWISH'][self.activation_method])\n",
        "      \n",
        "\n",
        "  def forward_pass(self, x, w1, b1, w2, b2):\n",
        "    \"\"\"\n",
        "    A forward pass function\n",
        "\n",
        "    Returns\n",
        "    -------    \n",
        "    out: \n",
        "\n",
        "    cache: \n",
        "\n",
        "    \"\"\"\n",
        "    h1     = None  # the activation after the first linear layer\n",
        "    y1, y2 = None, None  # outputs from the first & second linear layers\n",
        "\n",
        "    #############################################################################\n",
        "    # PLACE YOUR CODE HERE                                                      #\n",
        "    #############################################################################\n",
        "    # TODO: Design the fully-connected neural network and compute its forward.  #\n",
        "    #       pass output,                                                        #\n",
        "    #        Input - Linear layer - LeakyReLU - Linear layer.                   #\n",
        "    #       You have use predefined variables above                             #\n",
        "    #                                                                           #\n",
        "    # y1 =                                                                      #\n",
        "    #                                                                           #\n",
        "    # if self.activation_method == 0:                                           #\n",
        "    # h1 =  #ReLU                                                               #\n",
        "    # elif self.activation_method == 1:                                         #\n",
        "    # h1 =  #Leaky ReLU                                                         #\n",
        "    # else:                                                                     #\n",
        "    # h1 =  # SWISH                                                             #\n",
        "    #                                                                           #\n",
        "    # y2 =                                                                      #\n",
        "    #                                                                           #\n",
        "    #  END OF YOUR CODE                                                         #\n",
        "    #############################################################################\n",
        "    \n",
        "    out   = y2\n",
        "    cache = (y1, h1) # intermediate values\n",
        "\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "  def softmax_loss(self, x, y):\n",
        "    \"\"\"\n",
        "    Compute the loss and gradients for a softmax classifier\n",
        "\n",
        "    Returns\n",
        "    -------    \n",
        "    loss: \n",
        "\n",
        "    dx: \n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    #############################################################################\n",
        "    # PLACE YOUR CODE HERE                                                      #\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax classification loss and its gradient.           #\n",
        "    # The softmax loss is also known as cross-entropy loss.                     #\n",
        "    # p    =  # for stable softmax.                                             #\n",
        "    # summ = \n",
        "    # n    =                                                                    #\n",
        "    # loss =                                                                    #\n",
        "    #                                                                           #\n",
        "    #dx =                                                                       #\n",
        "    #  END OF YOUR CODE                                                         #  \n",
        "    #############################################################################\n",
        "\n",
        "    return loss, dx\n",
        "\n",
        "\n",
        "  def backward_pass(self, dY2_dLoss, x, w1, y1, h1, w2):\n",
        "    \"\"\"\n",
        "    A backward pass function\n",
        "\n",
        "    Returns\n",
        "    -------    \n",
        "    grads: \n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "\n",
        "    #############################################################################\n",
        "    # PLACE YOUR CODE HERE                                                      #\n",
        "    #############################################################################\n",
        "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
        "    # and biases. Store the results in the grads dictionary. For example,       #\n",
        "    # the gradient on W1 should be stored in grads['w1'] and be a matrix of same#\n",
        "    # size                                                                      #\n",
        "    #                                                                           #\n",
        "    #without regularization                                                     #\n",
        "    #grads['w2'] = np.dot(h1.T,dY2_dLoss)                                       # \n",
        "    #grads['b2'] = np.sum(dY2_dLoss, axis=0)                                    #\n",
        "    #                                                                           #\n",
        "    #if self.activation_method == 0:                                            #\n",
        "    #                                                                           #\n",
        "    #  dY1_dLoss = # ReLU                                                       #\n",
        "    #elif self.activation_method == 1:                                          #\n",
        "    #                                                                           #\n",
        "    #  dH1_dLoss =   # Leaky ReLU                                               # \n",
        "    #  inb =                                                                    #\n",
        "    #  inb[h1>0]=1                                                              #\n",
        "    #  dY1_dLoss =                                                              #\n",
        "    #else:                                                                      #\n",
        "    #                                                                           #\n",
        "    #  dH1_dLoss =   # SWISH                                                    #\n",
        "    #  swish_y1 =                                                               #\n",
        "    #  dY1_dH1 =                                                                #\n",
        "    #  dY1_dLoss =                                                              #  \n",
        "    #                                                                           #\n",
        "    #grads['w1'] =                                                              #\n",
        "    #grads['b1'] =                                                              #\n",
        "    #                                                                           #\n",
        "    #  END OF YOUR CODE                                                         #\n",
        "    #############################################################################\n",
        "    \n",
        "    return grads\n",
        "\n",
        "\n",
        "  def loss(self, x, y=None, regular=0.0, enable_margin=False):\n",
        "    \"\"\"\n",
        "    A loss function that returns the loss and gradients of the fully-connected \n",
        "    neural network. This function requires designing forward and backward passes.\n",
        "    \n",
        "    If y is None, it returns a matrix labelsof shape (n, c) where labels[i, c] \n",
        "    is the label score for class c on input x[i]. Otherwise, it returns a tuple\n",
        "    of loss and grads.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x:  matrix     \n",
        "        an input data of shape (n, d). Each x[i] is a training sample.\n",
        "    y:  vector     \n",
        "        a vector of training labels. Each y[i] is an integer in the range \n",
        "        0 <= y[i] < c. y[i] is the label for x[i]. If it is passed then we\n",
        "        return the loss and gradients.\n",
        "    regular: float\n",
        "        regularization strength.\n",
        "    enable_margin: Bool\n",
        "        enable to use soft-margin softmax\n",
        "    \n",
        "    Returns\n",
        "    -------    \n",
        "    loss: \n",
        "        Loss (data loss and regularization loss) for this batch of training \n",
        "        samples.\n",
        "    grads: \n",
        "        Dictionary mapping parameter names to gradients of those parameters with\n",
        "        respect to the loss function; has the same keys as self.params.\n",
        "    \"\"\"\n",
        "    # Variables\n",
        "    n, d   = x.shape # input dimensions\n",
        "    w1, b1 = self.params['w1'], self.params['b1']\n",
        "    w2, b2 = self.params['w2'], self.params['b2']\n",
        "    h1     = None  # the activation after the first linear layer\n",
        "    y1, y2 = None, None  # outputs from the first & second linear layers\n",
        "\n",
        "    # Compute the forward pass\n",
        "    out, cache = self.forward_pass(x,w1,b1,w2,b2)\n",
        "    y2       = out\n",
        "    (y1, h1) = cache\n",
        "\n",
        "    # If the targets are not given then jump out, we're done\n",
        "    if y is None:\n",
        "      return y2\n",
        "\n",
        "    # Compute the loss\n",
        "    loss, dY2_dLoss = self.softmax_loss(y2, y)\n",
        "\n",
        "    # Compute the backward pass\n",
        "    grads = self.backward_pass(dY2_dLoss, x, w1, y1, h1, w2)\n",
        "\n",
        "    #############################################################################\n",
        "    # (OPTION) PLACE YOUR CODE HERE                                             #\n",
        "    #############################################################################\n",
        "    # TODO: Implement weight regularization                                     #\n",
        "    #loss +=                                                                    #\n",
        "    #add regularization effect                                                  #\n",
        "    #grads['w2'] +=                                                             #\n",
        "    #grads['w1'] +=                                                             #\n",
        "    #  END OF YOUR CODE                                                         #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, grads\n",
        "\n",
        "\n",
        "  def train(self, x, y, x_v, y_v,\n",
        "            eta=1e-3, lamdba=0.95,\n",
        "            regular=1e-5, num_iters=50,\n",
        "            batch_size=100, verbose=False):\n",
        "    \"\"\"\n",
        "    Train this neural network using stochastic gradient descent.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array of shape (n, d) giving training data.\n",
        "    - y: A numpy array f shape (n,) giving training labels; y[i] = C means that\n",
        "      x[i] has label C, where 0 <= C < c.\n",
        "    - x_v: A numpy array of shape (n_v, d) giving validation data.\n",
        "    - y_v: A numpy array of shape (n_v,) giving validation labels.\n",
        "    - eta: Scalar giving learning rate for optimization.\n",
        "    - lamdba: Scalar giving factor used to decay the learning rate\n",
        "      after each epoch.\n",
        "    - regular: Scalar giving regularization strength.\n",
        "    - num_iters: Number of steps to take when optimizing.\n",
        "    - batch_size: Number of training examples to use per step.\n",
        "    - verbose: boolean; if true print progress during optimization.\n",
        "    \"\"\"\n",
        "    num_train = x.shape[0]\n",
        "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "    # Use Stochastic Gradient Descent (SGD) to optimize the parameters in \n",
        "    # self.model\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for it in range(num_iters):\n",
        "      x_batch = None\n",
        "      y_batch = None\n",
        "\n",
        "      #########################################################################\n",
        "      # PLACE YOUR CODE HERE                                                  #\n",
        "      #########################################################################\n",
        "      # TODO: Create a random minibatch of training data and labels, storing  #\n",
        "      # them in x_batch and y_batch respectively.                             #\n",
        "      #                                                                       #\n",
        "      #idx =                                                                  #\n",
        "      #x_batch =                                                              #\n",
        "      #y_batch =                                                              #\n",
        "      #                                                                       #\n",
        "      #  END OF YOUR CODE                                                     #\n",
        "      #########################################################################\n",
        "\n",
        "      # Compute loss and gradients using the current minibatch\n",
        "      loss, grads = self.loss(x_batch, y=y_batch, regular=regular)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      #########################################################################\n",
        "      # PLACE YOUR CODE HERE                                                  #\n",
        "      #########################################################################\n",
        "      # TODO: Update the parameters of the network stored in self.params by   #\n",
        "      # using the gradients in the grads dictionary. For that, use stochastic #\n",
        "      # gradient descent.                                                     #\n",
        "      #                                                                       #\n",
        "      #self.params['w1'] +=                                                   #\n",
        "      #self.params['w2'] +=                                                   #\n",
        "      #self.params['b1'] +=                                                   #\n",
        "      #self.params['b2'] +=                                                   #\n",
        "      #                                                                       #\n",
        "      #  END OF YOUR CODE                                                     #\n",
        "      #########################################################################\n",
        "\n",
        "      if verbose and it % 100 == 0:\n",
        "        print('The #iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "      # Every epoch, check train and val accuracy and decay learning rate.\n",
        "      if it % iterations_per_epoch == 0:\n",
        "        # Check accuracy\n",
        "        train_acc = (self.predict(x_batch) == y_batch).mean()\n",
        "        val_acc = (self.predict(x_v) == y_v).mean()\n",
        "        train_acc_history.append(train_acc)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        # Decay learning rate\n",
        "        eta *= lamdba\n",
        "\n",
        "    return {\n",
        "      'loss_history': loss_history,\n",
        "      'train_acc_history': train_acc_history,\n",
        "      'val_acc_history': val_acc_history,\n",
        "    }\n",
        "\n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    Use the trained weights of this MLP network to predict labels for\n",
        "    data points. For each data point we predict labels for each of the C\n",
        "    classes, and assign each data point to the class with the highest label \n",
        "    score.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array of shape (n, d) giving n d-dimensional data points to\n",
        "      classify.\n",
        "\n",
        "    Returns:\n",
        "    - y_pr: A numpy array of shape (n,) giving predicted labels for each of\n",
        "      the elements of x. For all i, y_pred[i] = c means that x[i] is predicted\n",
        "      to have class C, where 0 <= C < c.\n",
        "    \"\"\"\n",
        "    y_pr = None\n",
        "\n",
        "    ###########################################################################\n",
        "    # PLACE YOUR CODE HERE                                                    #\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the predict function                                    #\n",
        "    #out, _ =                                                                 #\n",
        "    #y_pr =                                                                   #\n",
        "    #                                                                         #\n",
        "    # END OF YOUR CODE                                                        #\n",
        "    ###########################################################################\n",
        "\n",
        "    return y_pr\n"
      ],
      "metadata": {
        "id": "GdQWZt82E1zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a model\n",
        "To train our model, we will use SGD with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
      ],
      "metadata": {
        "id": "Xsvt-C2U5O-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "input_size = 32 * 32 * 3\n",
        "hidden_size = 64 \n",
        "num_classes = 10\n",
        "activation = 'ReLU' # Select one in [ReLU, LeakyReLU, SWISH]\n",
        "net_mlp = MLP(input_size, hidden_size, num_classes, activation) \n",
        "\n",
        "# Train the network\n",
        "stats = net_mlp.train(X_train, y_train, X_val, y_val,\n",
        "            num_iters=1000, batch_size=200,\n",
        "            eta=1e-3, lamdba=0.95,\n",
        "            regular=0.5, verbose=True)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_acc = (net_mlp.predict(X_val) == y_val).mean()\n",
        "print('Validation accuracy: ', val_acc)"
      ],
      "metadata": {
        "id": "vCKu0J3wJLlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n",
        "\n",
        "\n",
        "You have to plot the loss function and the accuracies on the training and validation sets.Also, visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."
      ],
      "metadata": {
        "id": "zY_gDcL75pZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot the loss function and train / validation accuracies\n",
        "def showStats(stats):\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(stats['loss_history'])\n",
        "    plt.title('Loss history')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(stats['train_acc_history'], label='train')\n",
        "    plt.plot(stats['val_acc_history'], label='val')\n",
        "    plt.title('Classification accuracy history')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Clasification accuracy')\n",
        "    plt.show()\n",
        "    \n",
        "showStats(stats)"
      ],
      "metadata": {
        "id": "cc9MjcEUJWO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_grid(xs, ubound=255.0, padding=1):\n",
        "  \"\"\"\n",
        "  Reshape an image data of 4D tensor to a grid for the better understanding and visualization.\n",
        "\n",
        "  Inputs:\n",
        "  - xs: Data of shape (n, h, w, c)\n",
        "  - ubound: Output grid will have values scaled to the range [0, ubound]\n",
        "  - padding: The number of blank pixels between elements of the grid\n",
        "  \"\"\"\n",
        "  (n, h, w, c) = xs.shape\n",
        "  grid_size = int(ceil(sqrt(n)))\n",
        "  grid_height = h * grid_size + padding * (grid_size - 1)\n",
        "  grid_width = w * grid_size + padding * (grid_size - 1)\n",
        "  grid = np.zeros((grid_height, grid_width, c))\n",
        "  next_idx = 0\n",
        "  y0, y1 = 0, h\n",
        "  for y in range(grid_size):\n",
        "    x0, x1 = 0, w\n",
        "    for x in range(grid_size):\n",
        "      if next_idx < n:\n",
        "        img = xs[next_idx]\n",
        "        low, high = np.min(img), np.max(img)\n",
        "        grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
        "        # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
        "        next_idx += 1\n",
        "      x0 += w + padding\n",
        "      x1 += w + padding\n",
        "    y0 += h + padding\n",
        "    y1 += h + padding\n",
        "  # grid_max = np.max(grid)\n",
        "  # grid_min = np.min(grid)\n",
        "  # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n",
        "  return grid"
      ],
      "metadata": {
        "id": "Qhxfn83WyfAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from math import *\n",
        "\n",
        "def show_net_weights(net):\n",
        "  w1 = net_mlp.params['w1']\n",
        "\n",
        "  ###########################################################################\n",
        "  # PLACE YOUR CODE HERE                                                    #\n",
        "  ###########################################################################\n",
        "  # TODO: Implement the weight visualization                                #\n",
        "  # w1 =                                                                    #\n",
        "  # END OF YOUR CODE                                                        #\n",
        "  ###########################################################################\n",
        "\n",
        "  plt.gca().axis('off')\n",
        "  plt.show()\n",
        "\n",
        "show_net_weights(net_mlp)"
      ],
      "metadata": {
        "id": "eHTZdjiyJv5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tune your hyperparameters\n",
        "\n",
        "We will tune the hyperparameters by developing intuition for how they affect the final performance. By doing so, we want you to get a lot of practice. Below, you should experiment with different values of various hyperparameters, including learning rate, number of hidden layers, and regularization strength (Option). \n",
        "\n",
        "**Expected results**. You should be aim to achieve a classification accuracy of greater than 49.5% on the validation set (with ReLU). Our best network gets over 52% on the validation set.\n"
      ],
      "metadata": {
        "id": "lSIpKpJ36bb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tune hyperparameters using the validation set. Store your best trained  \n",
        "model in best_net.                                                                                  \n",
        "\"\"\"\n",
        "from enum import EnumMeta\n",
        "import numpy as np\n",
        "\n",
        "best_net = None # store the best model into this \n",
        "best_val = -1\n",
        "results={}\n",
        "output_size = 10\n",
        "hidden_layer_sizes =[64, 128, 256, 1024]\n",
        "eta = [5e-4, 1e-3, 2e-3, 5e-3]                 #Use different values from: np.power(10,np.random.uniform(-7,-2,10)) \n",
        "regularization_strengths =[0.8] #Use different values from: np.power(10,np.random.uniform(-3,1,10))\n",
        "activation = 'ReLU' # Select one in [ReLU, LeakyReLU, SWISH]\n",
        "\n",
        "for hs in hidden_layer_sizes:\n",
        "    for lr in eta:\n",
        "        for rs in regularization_strengths:\n",
        "\n",
        "            ####################################################################\n",
        "            # PLACE YOUR CODE HERE                                             #\n",
        "            ####################################################################          \n",
        "            # TODO:                                                            #\n",
        "            # Use the validation set to check the validataion accuracy per     #\n",
        "            # a pair of parameters.                                            #\n",
        "            # Save the best trained softmax classifer.                         #\n",
        "            #nn =                                                              #\n",
        "            #nnstats =                                                         #\n",
        "            #y_train_pred =                                                    #\n",
        "            #print(y_train_pred)                                               #\n",
        "            #ta =                                                              #\n",
        "            #y_val_pred =                                                      #\n",
        "            #va =                                                              #\n",
        "            #if va > best_val:                                                 #\n",
        "            #    best_val =                                                    #\n",
        "            #    best_net=                                                     #\n",
        "            #    best_stats =                                                  #\n",
        "            #                                                                  #\n",
        "            # END OF YOUR CODE                                                 #\n",
        "            ####################################################################\n",
        "\n",
        "            # Save the results\n",
        "            # ta: training accuracy\n",
        "            # va: validation accuracy\n",
        "            results[(hs,lr,rs)] = (ta,va)\n",
        "\n",
        "    \n",
        "# Print out results.\n",
        "for hs,lr, regular in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(hs,lr, regular)]\n",
        "    print('hs %e lr %e regular %e train accuracy: %f val accuracy: %f' % (\n",
        "                hs,lr, regular, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
        "showStats(best_stats)\n"
      ],
      "metadata": {
        "id": "z9XfG-CSK1lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_net_weights(best_net)"
      ],
      "metadata": {
        "id": "Qb01MQ_7K9wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
        "print('Test accuracy: ', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvEpV8Y7yncl",
        "outputId": "e0a8a51d-b937-48e6-f1dc-6d73d8058395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.479\n"
          ]
        }
      ]
    }
  ]
}